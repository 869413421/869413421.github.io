<!DOCTYPE html>






































<html
  class="not-ready text-sm lg:text-base"
  style="--bg: #fff"
  lang="en-zh"
>
  <head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta
    name="viewport"
    content="width=device-width, initial-scale=1, shrink-to-fit=no"
  />

  
  <title>Transformer 学习之路 - 检索机器人实战 - 清水泥沙</title>

  
  <meta name="theme-color" />
  
  <meta name="description" content="Transformer 学习之路 - 检索机器人实战 在自然语言处理（NLP）领域，Transformer 模型已经成为许多任务的核心工具。今天，我们将通过一个实际案例——构建一个检索机器人，来深入探讨 Transformer 的应用。这个机器人能够根据用户的问题，从 FAQ 数据集中检索出最相关的答案。
1. 读取 FAQ 数据 首先，我们需要准备好 FAQ 数据集。这里我们使用了一个法律相关的 FAQ 数据集 law_faq.csv。
import pandas as pd data = pd.read_csv(&#34;/content/drive/MyDrive/ai-learning/2.NLP Task/06-retrieval_chatbot/law_faq.csv&#34;) data.head() 为什么需要这一步？ FAQ 数据集是我们机器人的“知识库”，它包含了常见问题及其对应的答案。读取数据后，我们才能进行后续的处理和检索。
2. 加载预训练模型 接下来，我们需要加载预训练的 Transformer 模型。这里我们使用了 DualModel 和 AutoTokenizer。
from dual_model import DualModel from transformers import AutoTokenizer dual_model = DualModel.from_pretrained(dual_model_path) dual_model.cuda() dual_model.eval() tokenizer = AutoTokenizer.from_pretrained(&#34;hfl/chinese-macbert-base&#34;) 为什么需要这一步？ 预训练模型已经在大规模数据上进行了训练，能够捕捉到丰富的语言特征。加载这些模型后，我们可以直接使用它们来对文本进行编码，而不需要从头开始训练。
3. 将问题编码转换为向量 为了进行检索，我们需要将 FAQ 中的问题转换为向量表示。这里我们使用了 DualModel 的 BERT 模型来生成向量。
import torch from tqdm import tqdm questions = data[&#34;title&#34;]." />
  <meta
    name="author"
    content=""
  />
  

  
  
  
  
  
  
  <link rel="preload stylesheet" as="style" href="https://869413421.github.io/main.min.css" />

  

  
     
  <link rel="preload" as="image" href="https://869413421.github.io/theme.png" />

  
  
  
  <link rel="preload" as="image" href="https://www.gravatar.com/avatar/6fd8df4abe41f17fd8e2dd7d97b5cc8c?s=160&amp;d=identicon" />
  
  

  
  <link rel="preload" as="image" href="https://869413421.github.io/github.svg" />
  
  <link rel="preload" as="image" href="https://869413421.github.io/rss.svg" />
  

  
  <link rel="icon" href="https://869413421.github.io/favicon.ico" />
  <link rel="apple-touch-icon" href="https://869413421.github.io/apple-touch-icon.png" />

  
  <meta name="generator" content="Hugo 0.110.0">

  
  

  
  
  
  
  
  
  
  
  
  <meta property="og:title" content="Transformer 学习之路 - 检索机器人实战" />
<meta property="og:description" content="深入解析如何利用 Transformer 技术构建检索机器人，涵盖数据读取、模型加载、向量编码、索引创建与匹配等关键步骤。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://869413421.github.io/post/transformer/retrieval_chatbot/" /><meta property="article:section" content="post" />
<meta property="article:published_time" content="2024-04-19T17:35:18+08:00" />
<meta property="article:modified_time" content="2024-04-19T17:35:18+08:00" />

  
  <meta itemprop="name" content="Transformer 学习之路 - 检索机器人实战">
<meta itemprop="description" content="深入解析如何利用 Transformer 技术构建检索机器人，涵盖数据读取、模型加载、向量编码、索引创建与匹配等关键步骤。"><meta itemprop="datePublished" content="2024-04-19T17:35:18+08:00" />
<meta itemprop="dateModified" content="2024-04-19T17:35:18+08:00" />
<meta itemprop="wordCount" content="274">
<meta itemprop="keywords" content="" />
  
  <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Transformer 学习之路 - 检索机器人实战"/>
<meta name="twitter:description" content="深入解析如何利用 Transformer 技术构建检索机器人，涵盖数据读取、模型加载、向量编码、索引创建与匹配等关键步骤。"/>

  
  
</head>

  <body class="text-black duration-200 ease-out dark:text-white">
    <header class="mx-auto flex h-[5rem] max-w-3xl px-8 lg:justify-center">
  <div class="relative z-50 mr-auto flex items-center">
    <a
      class="-translate-x-[1px] -translate-y-0.5 text-3xl font-bold"
      href="https://869413421.github.io/"
      >清水泥沙</a
    >
    <a
      class="btn-dark ml-6 h-6 w-6 shrink-0 cursor-pointer [background:url(./theme.svg)_left_center/cover_no-repeat] dark:invert dark:[background-position:right]"
    ></a>
  </div>

  <a
    class="btn-menu relative z-50 -mr-8 flex h-[5rem] w-[5rem] shrink-0 cursor-pointer flex-col items-center justify-center gap-2.5 lg:hidden"
  ></a>

  
  <script>
    
    const htmlClass = document.documentElement.classList;
    setTimeout(() => {
      htmlClass.remove('not-ready');
    }, 10);

    
    const btnMenu = document.querySelector('.btn-menu');
    btnMenu.addEventListener('click', () => {
      htmlClass.toggle('open');
    });

    
    const metaTheme = document.querySelector('meta[name="theme-color"]');
    const lightBg = `"#fff"`.replace(/"/g, '');
    const setDark = (isDark) => {
      metaTheme.setAttribute('content', isDark ? '#000' : lightBg);
      htmlClass[isDark ? 'add' : 'remove']('dark');
      localStorage.setItem('dark', isDark);
    };

    
    const darkScheme = window.matchMedia('(prefers-color-scheme: dark)');
    if (htmlClass.contains('dark')) {
      setDark(true);
    } else {
      const darkVal = localStorage.getItem('dark');
      setDark(darkVal ? darkVal === 'true' : darkScheme.matches);
    }

    
    darkScheme.addEventListener('change', (event) => {
      setDark(event.matches);
    });

    
    const btnDark = document.querySelector('.btn-dark');
    btnDark.addEventListener('click', () => {
      setDark(localStorage.getItem('dark') !== 'true');
    });
  </script>

  <div
    class="nav-wrapper fixed inset-x-0 top-full z-40 flex h-full select-none flex-col justify-center pb-16 duration-200 dark:bg-black lg:static lg:h-auto lg:flex-row lg:!bg-transparent lg:pb-0 lg:transition-none"
  >
    
    
    <nav class="lg:ml-12 lg:flex lg:flex-row lg:items-center lg:space-x-6">
      
      <a
        class="block text-center text-2xl leading-[5rem] lg:text-base lg:font-normal"
        href="/tags"
        >标签</a
      >
      
      <a
        class="block text-center text-2xl leading-[5rem] lg:text-base lg:font-normal"
        href="/categories"
        >分类</a
      >
      
      <a
        class="block text-center text-2xl leading-[5rem] lg:text-base lg:font-normal"
        href="/about/"
        >个人简历</a
      >
      
    </nav>
    

    
    <nav
      class="mt-12 flex justify-center space-x-10 dark:invert lg:mt-0 lg:ml-12 lg:items-center lg:space-x-6"
    >
      
      <a
        class="h-8 w-8 [background:var(--url)_center_center/cover_no-repeat] lg:h-6 lg:w-6"
        style="--url: url(./github.svg)"
        href=" https://github.com/869413421 "
        target="_blank"
      ></a>
      
      <a
        class="h-8 w-8 [background:var(--url)_center_center/cover_no-repeat] lg:h-6 lg:w-6"
        style="--url: url(./rss.svg)"
        href=" https://869413421.github.io/index.xml "
        target="_blank"
      ></a>
      
    </nav>
    
  </div>
</header>


    <main
      class="prose prose-neutral relative mx-auto min-h-[calc(100%-10rem)] max-w-3xl px-8 pt-20 pb-32 dark:prose-invert"
    >
      

<article>
  <header class="mb-20">
    <h1 class="!my-0 pb-2.5">Transformer 学习之路 - 检索机器人实战</h1>

    
    <div class="text-sm opacity-60">
      
      <time>Apr 19, 2024</time>
      
      
    </div>
    
  </header>

  <section><h1 id="transformer-学习之路---检索机器人实战">Transformer 学习之路 - 检索机器人实战</h1>
<p>在自然语言处理（NLP）领域，Transformer 模型已经成为许多任务的核心工具。今天，我们将通过一个实际案例——构建一个<strong>检索机器人</strong>，来深入探讨 Transformer 的应用。这个机器人能够根据用户的问题，从 FAQ 数据集中检索出最相关的答案。</p>
<h2 id="1-读取-faq-数据">1. 读取 FAQ 数据</h2>
<p>首先，我们需要准备好 FAQ 数据集。这里我们使用了一个法律相关的 FAQ 数据集 <code>law_faq.csv</code>。</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> pandas <span style="color:#66d9ef">as</span> pd
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>data <span style="color:#f92672">=</span> pd<span style="color:#f92672">.</span>read_csv(<span style="color:#e6db74">&#34;/content/drive/MyDrive/ai-learning/2.NLP Task/06-retrieval_chatbot/law_faq.csv&#34;</span>)
</span></span><span style="display:flex;"><span>data<span style="color:#f92672">.</span>head()
</span></span></code></pre></div><h3 id="为什么需要这一步">为什么需要这一步？</h3>
<p>FAQ 数据集是我们机器人的“知识库”，它包含了常见问题及其对应的答案。读取数据后，我们才能进行后续的处理和检索。</p>
<h2 id="2-加载预训练模型">2. 加载预训练模型</h2>
<p>接下来，我们需要加载预训练的 Transformer 模型。这里我们使用了 <code>DualModel</code> 和 <code>AutoTokenizer</code>。</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> dual_model <span style="color:#f92672">import</span> DualModel
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> transformers <span style="color:#f92672">import</span> AutoTokenizer
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>dual_model <span style="color:#f92672">=</span> DualModel<span style="color:#f92672">.</span>from_pretrained(dual_model_path)
</span></span><span style="display:flex;"><span>dual_model<span style="color:#f92672">.</span>cuda()
</span></span><span style="display:flex;"><span>dual_model<span style="color:#f92672">.</span>eval()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>tokenizer <span style="color:#f92672">=</span> AutoTokenizer<span style="color:#f92672">.</span>from_pretrained(<span style="color:#e6db74">&#34;hfl/chinese-macbert-base&#34;</span>)
</span></span></code></pre></div><h3 id="为什么需要这一步-1">为什么需要这一步？</h3>
<p>预训练模型已经在大规模数据上进行了训练，能够捕捉到丰富的语言特征。加载这些模型后，我们可以直接使用它们来对文本进行编码，而不需要从头开始训练。</p>
<h2 id="3-将问题编码转换为向量">3. 将问题编码转换为向量</h2>
<p>为了进行检索，我们需要将 FAQ 中的问题转换为向量表示。这里我们使用了 <code>DualModel</code> 的 BERT 模型来生成向量。</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> torch
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> tqdm <span style="color:#f92672">import</span> tqdm
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>questions <span style="color:#f92672">=</span> data[<span style="color:#e6db74">&#34;title&#34;</span>]<span style="color:#f92672">.</span>to_list()
</span></span><span style="display:flex;"><span>vectors <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">with</span> torch<span style="color:#f92672">.</span>inference_mode():
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> tqdm(range(<span style="color:#ae81ff">0</span>, len(questions), <span style="color:#ae81ff">32</span>)):
</span></span><span style="display:flex;"><span>        batch_sens <span style="color:#f92672">=</span> questions[i: i <span style="color:#f92672">+</span> <span style="color:#ae81ff">32</span>]
</span></span><span style="display:flex;"><span>        inputs <span style="color:#f92672">=</span> tokenizer(batch_sens, return_tensors<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;pt&#34;</span>, padding<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>, max_length<span style="color:#f92672">=</span><span style="color:#ae81ff">128</span>, truncation<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        inputs <span style="color:#f92672">=</span> {k: v<span style="color:#f92672">.</span>to(dual_model<span style="color:#f92672">.</span>device) <span style="color:#66d9ef">for</span> k, v <span style="color:#f92672">in</span> inputs<span style="color:#f92672">.</span>items()}
</span></span><span style="display:flex;"><span>        vector <span style="color:#f92672">=</span> dual_model<span style="color:#f92672">.</span>bert(<span style="color:#f92672">**</span>inputs)[<span style="color:#ae81ff">1</span>]
</span></span><span style="display:flex;"><span>        vectors<span style="color:#f92672">.</span>append(vector)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>vectors <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>concat(vectors, dim<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)<span style="color:#f92672">.</span>cpu()<span style="color:#f92672">.</span>numpy()
</span></span><span style="display:flex;"><span>vectors<span style="color:#f92672">.</span>shape
</span></span></code></pre></div><h3 id="为什么需要这一步-2">为什么需要这一步？</h3>
<p>将文本转换为向量后，我们可以通过计算向量之间的相似度来找到最相关的问题。这种向量化的表示方式使得检索过程更加高效和准确。</p>
<h2 id="4-创建索引存储向量">4. 创建索引，存储向量</h2>
<p>为了快速检索，我们需要将生成的向量存储在索引中。这里我们使用了 <code>faiss</code> 库来创建和存储索引。</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> faiss
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>index <span style="color:#f92672">=</span> faiss<span style="color:#f92672">.</span>IndexFlatIP(<span style="color:#ae81ff">768</span>)
</span></span><span style="display:flex;"><span>faiss<span style="color:#f92672">.</span>normalize_L2(vectors)
</span></span><span style="display:flex;"><span>index<span style="color:#f92672">.</span>add(vectors)
</span></span></code></pre></div><h3 id="为什么需要这一步-3">为什么需要这一步？</h3>
<p>索引能够加速向量检索的过程。通过将向量存储在索引中，我们可以在毫秒级别内找到与用户问题最相关的 FAQ。</p>
<h2 id="5-对用户提问进行编码">5. 对用户提问进行编码</h2>
<p>当用户提出问题时，我们同样需要将其转换为向量。</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>question <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;寻衅滋事&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">with</span> torch<span style="color:#f92672">.</span>inference_mode():
</span></span><span style="display:flex;"><span>    inputs <span style="color:#f92672">=</span> tokenizer(question, return_tensors<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;pt&#34;</span>, padding<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>, max_length<span style="color:#f92672">=</span><span style="color:#ae81ff">128</span>, truncation<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>    inputs <span style="color:#f92672">=</span> {k: v<span style="color:#f92672">.</span>to(dual_model<span style="color:#f92672">.</span>device) <span style="color:#66d9ef">for</span> k, v <span style="color:#f92672">in</span> inputs<span style="color:#f92672">.</span>items()}
</span></span><span style="display:flex;"><span>    vector <span style="color:#f92672">=</span> dual_model<span style="color:#f92672">.</span>bert(<span style="color:#f92672">**</span>inputs)[<span style="color:#ae81ff">1</span>]
</span></span><span style="display:flex;"><span>    q_vector <span style="color:#f92672">=</span> vector<span style="color:#f92672">.</span>cpu()<span style="color:#f92672">.</span>numpy()
</span></span></code></pre></div><h3 id="为什么需要这一步-4">为什么需要这一步？</h3>
<p>用户问题的向量表示是检索的基础。只有将问题转换为向量，我们才能通过索引找到最相关的问题。</p>
<h2 id="6-向量匹配根据向量召回">6. 向量匹配（根据向量召回）</h2>
<p>接下来，我们通过 <code>faiss</code> 索引来找到与用户问题最相关的 FAQ。</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>faiss<span style="color:#f92672">.</span>normalize_L2(q_vector)
</span></span><span style="display:flex;"><span>scores, indexes <span style="color:#f92672">=</span> index<span style="color:#f92672">.</span>search(q_vector, <span style="color:#ae81ff">10</span>)
</span></span><span style="display:flex;"><span>topk_result <span style="color:#f92672">=</span> data<span style="color:#f92672">.</span>values[indexes[<span style="color:#ae81ff">0</span>]<span style="color:#f92672">.</span>tolist()]
</span></span><span style="display:flex;"><span>topk_result[:, <span style="color:#ae81ff">0</span>]
</span></span></code></pre></div><h3 id="为什么需要这一步-5">为什么需要这一步？</h3>
<p>向量匹配是检索的核心步骤。通过计算用户问题向量与 FAQ 向量之间的相似度，我们可以找到最相关的问题。</p>
<h2 id="7-加载交互模型用于匹配最相似的-faq">7. 加载交互模型，用于匹配最相似的 FAQ</h2>
<p>为了进一步提高检索的准确性，我们加载了一个交互模型 <code>AutoModelForSequenceClassification</code>。</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> transformers <span style="color:#f92672">import</span> AutoModelForSequenceClassification
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>corss_model <span style="color:#f92672">=</span> AutoModelForSequenceClassification<span style="color:#f92672">.</span>from_pretrained(cross_model_path, num_labels<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>corss_model <span style="color:#f92672">=</span> corss_model<span style="color:#f92672">.</span>cuda()
</span></span><span style="display:flex;"><span>corss_model<span style="color:#f92672">.</span>eval()
</span></span></code></pre></div><h3 id="为什么需要这一步-6">为什么需要这一步？</h3>
<p>交互模型能够更精确地计算问题与候选答案之间的匹配度。通过这个模型，我们可以从候选答案中选择最合适的答案。</p>
<h2 id="8-最终预测">8. 最终预测</h2>
<p>最后，我们使用交互模型对候选答案进行打分，并选择最合适的答案。</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>canidate <span style="color:#f92672">=</span> topk_result[:, <span style="color:#ae81ff">0</span>]<span style="color:#f92672">.</span>tolist()
</span></span><span style="display:flex;"><span>ques <span style="color:#f92672">=</span> [question] <span style="color:#f92672">*</span> len(canidate)
</span></span><span style="display:flex;"><span>inputs <span style="color:#f92672">=</span> tokenizer(ques, canidate, return_tensors<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;pt&#34;</span>, padding<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>, max_length<span style="color:#f92672">=</span><span style="color:#ae81ff">128</span>, truncation<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>inputs <span style="color:#f92672">=</span> {k: v<span style="color:#f92672">.</span>to(corss_model<span style="color:#f92672">.</span>device) <span style="color:#66d9ef">for</span> k, v <span style="color:#f92672">in</span> inputs<span style="color:#f92672">.</span>items()}
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">with</span> torch<span style="color:#f92672">.</span>inference_mode():
</span></span><span style="display:flex;"><span>    logits <span style="color:#f92672">=</span> corss_model(<span style="color:#f92672">**</span>inputs)<span style="color:#f92672">.</span>logits<span style="color:#f92672">.</span>squeeze()
</span></span><span style="display:flex;"><span>    result <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>argmax(logits, dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>canidate_answer <span style="color:#f92672">=</span> topk_result[:, <span style="color:#ae81ff">1</span>]<span style="color:#f92672">.</span>tolist()
</span></span><span style="display:flex;"><span>match_quesiton <span style="color:#f92672">=</span> canidate[result<span style="color:#f92672">.</span>item()]
</span></span><span style="display:flex;"><span>final_answer <span style="color:#f92672">=</span> canidate_answer[result<span style="color:#f92672">.</span>item()]
</span></span><span style="display:flex;"><span>match_quesiton, final_answer
</span></span></code></pre></div><h3 id="为什么需要这一步-7">为什么需要这一步？</h3>
<p>最终预测是检索机器人的最后一步。通过交互模型的选择，我们能够确保返回给用户的答案是最准确和相关的。</p>
<h2 id="总结">总结</h2>
<p>通过以上步骤，我们成功构建了一个基于 Transformer 的检索机器人。这个机器人能够从 FAQ 数据集中快速找到与用户问题最相关的答案。整个过程涉及了数据读取、模型加载、向量编码、索引创建与匹配等多个关键步骤，展示了 Transformer 在 NLP 任务中的强大能力。</p>
<p>希望这篇文章能够帮助你更好地理解 Transformer 的应用，并为你的 NLP 项目提供一些灵感。如果你有任何问题或建议，欢迎在评论区留言讨论！</p>
</section>

  
  

  
  
  
  <nav class="mt-24 flex rounded-lg bg-black/[3%] text-lg dark:bg-white/[8%]">
    
    <a
      class="flex w-1/2 items-center rounded-l-md p-6 pr-3 no-underline hover:bg-black/[2%]"
      href="https://869413421.github.io/post/transformer/mrc_simple_version-checkpoint/"
      ><span class="mr-1.5">←</span><span>Transformer 学习之路 - 机器阅读理解任务实现</span></a
    >
    
    
    <a
      class="ml-auto flex w-1/2 items-center justify-end rounded-r-md p-6 pl-3 no-underline hover:bg-black/[2%]"
      href="https://869413421.github.io/post/transformer/classification/"
      ><span>Transformer 学习之路 - 法律标题文本分类实战</span><span class="ml-1.5">→</span></a
    >
    
  </nav>
  

  
  

  
  
</article>


    </main>

    <footer
  class="opaco mx-auto flex h-[5rem] max-w-3xl items-center px-8 text-[0.9em] opacity-60"
>
  <div class="mr-auto">
    &copy; 2025
    <a class="link" href="https://869413421.github.io/">清水泥沙</a>
  </div>
</footer>

  </body>
</html>
