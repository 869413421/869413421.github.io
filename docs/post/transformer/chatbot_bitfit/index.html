<!DOCTYPE html>






































<html
  class="not-ready text-sm lg:text-base"
  style="--bg: #fff"
  lang="en-zh"
>
  <head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta
    name="viewport"
    content="width=device-width, initial-scale=1, shrink-to-fit=no"
  />

  
  <title>Transformer 学习之路 - BitFit 实战与深入解析 - 清水泥沙</title>

  
  <meta name="theme-color" />
  
  <meta name="description" content="BitFit 实战与深入解析 BitFit 是一种参数高效微调（PEFT, Parameter-Efficient Fine-Tuning）方法，特别适用于大型语言模型的微调。它的核心思想是只微调模型的偏置项（bias），而冻结其他参数。这样可以减少微调所需的参数数量和存储成本，同时保持较高的性能。
BitFit 方法的基本概念 在深度学习模型中，每一层都有权重参数（weights）和偏置参数（biases）。在 BitFit 中：
只更新偏置参数：冻结模型的权重（weights），只调整偏置（biases）。 减少存储和计算需求：由于偏置项的数量远少于权重，这种方法大大降低了训练过程中的计算量和存储需求。 BitFit 的优缺点 优点 节省存储空间和计算资源：只更新偏置项，节省了大量的计算和存储成本。 迁移性强：由于权重参数不变，可以将微调后的偏置项应用到不同模型实例上，便于模型部署和版本管理。 适用于少量数据的任务：BitFit 在数据量较少的任务上表现尤为出色，能显著提高效率。 减少过拟合风险：只微调少量参数（偏置），可以避免模型在小数据集上过度拟合。 缺点 性能上限：BitFit 仅调整偏置参数，限制了模型的微调能力，因此在高复杂度或需要更深层次调整的任务中效果可能不及全面微调。 任务依赖性较高：BitFit 对某些任务（如高复杂度分类）可能不如全参数微调效果好。 局限性：BitFit 对需要大幅度知识迁移的任务（如极度不同的领域任务）效果有限，适合与预训练任务较相似的场景。 示例：使用 BitFit 微调情感分析模型 假设我们有一个预训练的 BERT 模型，并希望用少量的计算资源微调它来完成情感分类任务。以下是 BitFit 的实现步骤示例：
加载预训练模型，并将所有权重冻结。 解冻偏置项：只解冻每一层的偏置（bias），权重保持冻结。 训练模型：在情感分类数据集上训练，但只更新偏置项。 具体代码（使用 PyTorch 和 Hugging Face 的 Transformers 库）如下：
import torch from transformers import BertTokenizer, BertForSequenceClassification # 加载预训练模型和分词器 model = BertForSequenceClassification.from_pretrained(&#34;bert-base-uncased&#34;, num_labels=2) tokenizer = BertTokenizer.from_pretrained(&#34;bert-base-uncased&#34;) # 冻结模型的所有权重 for param in model.parameters(): param.requires_grad = False # 解冻偏置项（仅对偏置项设置 requires_grad 为 True） for name, param in model." />
  <meta
    name="author"
    content=""
  />
  

  
  
  
  
  
  
  <link rel="preload stylesheet" as="style" href="https://869413421.github.io/main.min.css" />

  

  
     
  <link rel="preload" as="image" href="https://869413421.github.io/theme.png" />

  
  
  
  <link rel="preload" as="image" href="https://www.gravatar.com/avatar/6fd8df4abe41f17fd8e2dd7d97b5cc8c?s=160&amp;d=identicon" />
  
  

  
  <link rel="preload" as="image" href="https://869413421.github.io/github.svg" />
  
  <link rel="preload" as="image" href="https://869413421.github.io/rss.svg" />
  

  
  <link rel="icon" href="https://869413421.github.io/favicon.ico" />
  <link rel="apple-touch-icon" href="https://869413421.github.io/apple-touch-icon.png" />

  
  <meta name="generator" content="Hugo 0.110.0">

  
  

  
  
  
  
  
  
  
  
  
  <meta property="og:title" content="Transformer 学习之路 - BitFit 实战与深入解析" />
<meta property="og:description" content="深入解析 BitFit 方法及其在 Transformer 模型中的应用，结合代码示例展示如何高效微调大型语言模型。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://869413421.github.io/post/transformer/chatbot_bitfit/" /><meta property="article:section" content="post" />
<meta property="article:published_time" content="2024-04-19T17:35:18+08:00" />
<meta property="article:modified_time" content="2024-04-19T17:35:18+08:00" />

  
  <meta itemprop="name" content="Transformer 学习之路 - BitFit 实战与深入解析">
<meta itemprop="description" content="深入解析 BitFit 方法及其在 Transformer 模型中的应用，结合代码示例展示如何高效微调大型语言模型。"><meta itemprop="datePublished" content="2024-04-19T17:35:18+08:00" />
<meta itemprop="dateModified" content="2024-04-19T17:35:18+08:00" />
<meta itemprop="wordCount" content="327">
<meta itemprop="keywords" content="" />
  
  <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Transformer 学习之路 - BitFit 实战与深入解析"/>
<meta name="twitter:description" content="深入解析 BitFit 方法及其在 Transformer 模型中的应用，结合代码示例展示如何高效微调大型语言模型。"/>

  
  
</head>

  <body class="text-black duration-200 ease-out dark:text-white">
    <header class="mx-auto flex h-[5rem] max-w-3xl px-8 lg:justify-center">
  <div class="relative z-50 mr-auto flex items-center">
    <a
      class="-translate-x-[1px] -translate-y-0.5 text-3xl font-bold"
      href="https://869413421.github.io/"
      >清水泥沙</a
    >
    <a
      class="btn-dark ml-6 h-6 w-6 shrink-0 cursor-pointer [background:url(./theme.svg)_left_center/cover_no-repeat] dark:invert dark:[background-position:right]"
    ></a>
  </div>

  <a
    class="btn-menu relative z-50 -mr-8 flex h-[5rem] w-[5rem] shrink-0 cursor-pointer flex-col items-center justify-center gap-2.5 lg:hidden"
  ></a>

  
  <script>
    
    const htmlClass = document.documentElement.classList;
    setTimeout(() => {
      htmlClass.remove('not-ready');
    }, 10);

    
    const btnMenu = document.querySelector('.btn-menu');
    btnMenu.addEventListener('click', () => {
      htmlClass.toggle('open');
    });

    
    const metaTheme = document.querySelector('meta[name="theme-color"]');
    const lightBg = `"#fff"`.replace(/"/g, '');
    const setDark = (isDark) => {
      metaTheme.setAttribute('content', isDark ? '#000' : lightBg);
      htmlClass[isDark ? 'add' : 'remove']('dark');
      localStorage.setItem('dark', isDark);
    };

    
    const darkScheme = window.matchMedia('(prefers-color-scheme: dark)');
    if (htmlClass.contains('dark')) {
      setDark(true);
    } else {
      const darkVal = localStorage.getItem('dark');
      setDark(darkVal ? darkVal === 'true' : darkScheme.matches);
    }

    
    darkScheme.addEventListener('change', (event) => {
      setDark(event.matches);
    });

    
    const btnDark = document.querySelector('.btn-dark');
    btnDark.addEventListener('click', () => {
      setDark(localStorage.getItem('dark') !== 'true');
    });
  </script>

  <div
    class="nav-wrapper fixed inset-x-0 top-full z-40 flex h-full select-none flex-col justify-center pb-16 duration-200 dark:bg-black lg:static lg:h-auto lg:flex-row lg:!bg-transparent lg:pb-0 lg:transition-none"
  >
    
    
    <nav class="lg:ml-12 lg:flex lg:flex-row lg:items-center lg:space-x-6">
      
      <a
        class="block text-center text-2xl leading-[5rem] lg:text-base lg:font-normal"
        href="/tags"
        >标签</a
      >
      
      <a
        class="block text-center text-2xl leading-[5rem] lg:text-base lg:font-normal"
        href="/categories"
        >分类</a
      >
      
      <a
        class="block text-center text-2xl leading-[5rem] lg:text-base lg:font-normal"
        href="/about/"
        >个人简历</a
      >
      
    </nav>
    

    
    <nav
      class="mt-12 flex justify-center space-x-10 dark:invert lg:mt-0 lg:ml-12 lg:items-center lg:space-x-6"
    >
      
      <a
        class="h-8 w-8 [background:var(--url)_center_center/cover_no-repeat] lg:h-6 lg:w-6"
        style="--url: url(./github.svg)"
        href=" https://github.com/869413421 "
        target="_blank"
      ></a>
      
      <a
        class="h-8 w-8 [background:var(--url)_center_center/cover_no-repeat] lg:h-6 lg:w-6"
        style="--url: url(./rss.svg)"
        href=" https://869413421.github.io/index.xml "
        target="_blank"
      ></a>
      
    </nav>
    
  </div>
</header>


    <main
      class="prose prose-neutral relative mx-auto min-h-[calc(100%-10rem)] max-w-3xl px-8 pt-20 pb-32 dark:prose-invert"
    >
      

<article>
  <header class="mb-20">
    <h1 class="!my-0 pb-2.5">Transformer 学习之路 - BitFit 实战与深入解析</h1>

    
    <div class="text-sm opacity-60">
      
      <time>Apr 19, 2024</time>
      
      
    </div>
    
  </header>

  <section><h1 id="bitfit-实战与深入解析">BitFit 实战与深入解析</h1>
<p><strong>BitFit</strong> 是一种参数高效微调（PEFT, Parameter-Efficient Fine-Tuning）方法，特别适用于大型语言模型的微调。它的核心思想是只微调模型的偏置项（bias），而冻结其他参数。这样可以减少微调所需的参数数量和存储成本，同时保持较高的性能。</p>
<h2 id="bitfit-方法的基本概念">BitFit 方法的基本概念</h2>
<p>在深度学习模型中，每一层都有权重参数（weights）和偏置参数（biases）。在 BitFit 中：</p>
<ol>
<li><strong>只更新偏置参数</strong>：冻结模型的权重（weights），只调整偏置（biases）。</li>
<li><strong>减少存储和计算需求</strong>：由于偏置项的数量远少于权重，这种方法大大降低了训练过程中的计算量和存储需求。</li>
</ol>
<h2 id="bitfit-的优缺点">BitFit 的优缺点</h2>
<h3 id="优点">优点</h3>
<ol>
<li><strong>节省存储空间和计算资源</strong>：只更新偏置项，节省了大量的计算和存储成本。</li>
<li><strong>迁移性强</strong>：由于权重参数不变，可以将微调后的偏置项应用到不同模型实例上，便于模型部署和版本管理。</li>
<li><strong>适用于少量数据的任务</strong>：BitFit 在数据量较少的任务上表现尤为出色，能显著提高效率。</li>
<li><strong>减少过拟合风险</strong>：只微调少量参数（偏置），可以避免模型在小数据集上过度拟合。</li>
</ol>
<h3 id="缺点">缺点</h3>
<ol>
<li><strong>性能上限</strong>：BitFit 仅调整偏置参数，限制了模型的微调能力，因此在高复杂度或需要更深层次调整的任务中效果可能不及全面微调。</li>
<li><strong>任务依赖性较高</strong>：BitFit 对某些任务（如高复杂度分类）可能不如全参数微调效果好。</li>
<li><strong>局限性</strong>：BitFit 对需要大幅度知识迁移的任务（如极度不同的领域任务）效果有限，适合与预训练任务较相似的场景。</li>
</ol>
<h2 id="示例使用-bitfit-微调情感分析模型">示例：使用 BitFit 微调情感分析模型</h2>
<p>假设我们有一个预训练的 BERT 模型，并希望用少量的计算资源微调它来完成情感分类任务。以下是 BitFit 的实现步骤示例：</p>
<ol>
<li><strong>加载预训练模型</strong>，并将所有权重冻结。</li>
<li><strong>解冻偏置项</strong>：只解冻每一层的偏置（<code>bias</code>），权重保持冻结。</li>
<li><strong>训练模型</strong>：在情感分类数据集上训练，但只更新偏置项。</li>
</ol>
<p>具体代码（使用 PyTorch 和 Hugging Face 的 Transformers 库）如下：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> torch
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> transformers <span style="color:#f92672">import</span> BertTokenizer, BertForSequenceClassification
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 加载预训练模型和分词器</span>
</span></span><span style="display:flex;"><span>model <span style="color:#f92672">=</span> BertForSequenceClassification<span style="color:#f92672">.</span>from_pretrained(<span style="color:#e6db74">&#34;bert-base-uncased&#34;</span>, num_labels<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>tokenizer <span style="color:#f92672">=</span> BertTokenizer<span style="color:#f92672">.</span>from_pretrained(<span style="color:#e6db74">&#34;bert-base-uncased&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 冻结模型的所有权重</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> param <span style="color:#f92672">in</span> model<span style="color:#f92672">.</span>parameters():
</span></span><span style="display:flex;"><span>    param<span style="color:#f92672">.</span>requires_grad <span style="color:#f92672">=</span> <span style="color:#66d9ef">False</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 解冻偏置项（仅对偏置项设置 requires_grad 为 True）</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> name, param <span style="color:#f92672">in</span> model<span style="color:#f92672">.</span>named_parameters():
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> <span style="color:#e6db74">&#34;bias&#34;</span> <span style="color:#f92672">in</span> name:
</span></span><span style="display:flex;"><span>        param<span style="color:#f92672">.</span>requires_grad <span style="color:#f92672">=</span> <span style="color:#66d9ef">True</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 准备数据</span>
</span></span><span style="display:flex;"><span>texts <span style="color:#f92672">=</span> [<span style="color:#e6db74">&#34;I love this product!&#34;</span>, <span style="color:#e6db74">&#34;This is terrible...&#34;</span>]
</span></span><span style="display:flex;"><span>labels <span style="color:#f92672">=</span> [<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">0</span>]  <span style="color:#75715e"># 假设 1 表示正面情感，0 表示负面情感</span>
</span></span><span style="display:flex;"><span>inputs <span style="color:#f92672">=</span> tokenizer(texts, return_tensors<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;pt&#34;</span>, padding<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>, truncation<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 定义损失函数和优化器</span>
</span></span><span style="display:flex;"><span>criterion <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>nn<span style="color:#f92672">.</span>CrossEntropyLoss()
</span></span><span style="display:flex;"><span>optimizer <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>optim<span style="color:#f92672">.</span>Adam(filter(<span style="color:#66d9ef">lambda</span> p: p<span style="color:#f92672">.</span>requires_grad, model<span style="color:#f92672">.</span>parameters()), lr<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-3</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 训练</span>
</span></span><span style="display:flex;"><span>model<span style="color:#f92672">.</span>train()
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> epoch <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">2</span>):
</span></span><span style="display:flex;"><span>    optimizer<span style="color:#f92672">.</span>zero_grad()
</span></span><span style="display:flex;"><span>    outputs <span style="color:#f92672">=</span> model(<span style="color:#f92672">**</span>inputs, labels<span style="color:#f92672">=</span>torch<span style="color:#f92672">.</span>tensor(labels))
</span></span><span style="display:flex;"><span>    loss <span style="color:#f92672">=</span> criterion(outputs<span style="color:#f92672">.</span>logits, torch<span style="color:#f92672">.</span>tensor(labels))
</span></span><span style="display:flex;"><span>    loss<span style="color:#f92672">.</span>backward()
</span></span><span style="display:flex;"><span>    optimizer<span style="color:#f92672">.</span>step()
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Epoch </span><span style="color:#e6db74">{</span>epoch<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span><span style="color:#e6db74">}</span><span style="color:#e6db74">, Loss: </span><span style="color:#e6db74">{</span>loss<span style="color:#f92672">.</span>item()<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span></code></pre></div><h3 id="解释">解释</h3>
<ul>
<li>在这个例子中，只有偏置项的参数会被优化，所有其他参数保持不变。</li>
<li>通过这种方式，BitFit 能在保持模型原有知识的同时，适应新的情感分类任务。</li>
</ul>
<h2 id="总结">总结</h2>
<p>BitFit 是一种有效的参数高效微调方法，适合资源有限和特定应用场景，在数据量较少的任务中表现出色。</p>
<h2 id="进一步实践bitfit-在大型语言模型中的应用">进一步实践：BitFit 在大型语言模型中的应用</h2>
<p>在实际应用中，BitFit 同样适用于大型语言模型的微调。以下是一个使用 BitFit 微调 BLOOM 模型的示例：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> datasets <span style="color:#f92672">import</span> Dataset
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> transformers <span style="color:#f92672">import</span> AutoTokenizer, AutoModelForCausalLM, DataCollatorForSeq2Seq, TrainingArguments, Trainer
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 加载数据集</span>
</span></span><span style="display:flex;"><span>ds <span style="color:#f92672">=</span> Dataset<span style="color:#f92672">.</span>load_from_disk(<span style="color:#e6db74">&#34;/content/drive/MyDrive/ai-learning/3-PEFT/data/alpaca_data_zh/&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 数据集预处理</span>
</span></span><span style="display:flex;"><span>tokenizer <span style="color:#f92672">=</span> AutoTokenizer<span style="color:#f92672">.</span>from_pretrained(<span style="color:#e6db74">&#34;Langboat/bloom-1b4-zh&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">process_func</span>(example):
</span></span><span style="display:flex;"><span>    MAX_LENGTH <span style="color:#f92672">=</span> <span style="color:#ae81ff">256</span>
</span></span><span style="display:flex;"><span>    instruction <span style="color:#f92672">=</span> tokenizer(<span style="color:#e6db74">&#34;</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#34;</span><span style="color:#f92672">.</span>join([<span style="color:#e6db74">&#34;Human: &#34;</span> <span style="color:#f92672">+</span> example[<span style="color:#e6db74">&#34;instruction&#34;</span>], example[<span style="color:#e6db74">&#34;input&#34;</span>]])<span style="color:#f92672">.</span>strip() <span style="color:#f92672">+</span> <span style="color:#e6db74">&#34;</span><span style="color:#ae81ff">\n\n</span><span style="color:#e6db74">Assistant: &#34;</span>)
</span></span><span style="display:flex;"><span>    response <span style="color:#f92672">=</span> tokenizer(example[<span style="color:#e6db74">&#34;output&#34;</span>] <span style="color:#f92672">+</span> tokenizer<span style="color:#f92672">.</span>eos_token)
</span></span><span style="display:flex;"><span>    input_ids <span style="color:#f92672">=</span> instruction[<span style="color:#e6db74">&#34;input_ids&#34;</span>] <span style="color:#f92672">+</span> response[<span style="color:#e6db74">&#34;input_ids&#34;</span>]
</span></span><span style="display:flex;"><span>    attention_mask <span style="color:#f92672">=</span> instruction[<span style="color:#e6db74">&#34;attention_mask&#34;</span>] <span style="color:#f92672">+</span> response[<span style="color:#e6db74">&#34;attention_mask&#34;</span>]
</span></span><span style="display:flex;"><span>    labels <span style="color:#f92672">=</span> [<span style="color:#f92672">-</span><span style="color:#ae81ff">100</span>] <span style="color:#f92672">*</span> len(instruction[<span style="color:#e6db74">&#34;input_ids&#34;</span>]) <span style="color:#f92672">+</span> response[<span style="color:#e6db74">&#34;input_ids&#34;</span>]
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> len(input_ids) <span style="color:#f92672">&gt;</span> MAX_LENGTH:
</span></span><span style="display:flex;"><span>        input_ids <span style="color:#f92672">=</span> input_ids[:MAX_LENGTH]
</span></span><span style="display:flex;"><span>        attention_mask <span style="color:#f92672">=</span> attention_mask[:MAX_LENGTH]
</span></span><span style="display:flex;"><span>        labels <span style="color:#f92672">=</span> labels[:MAX_LENGTH]
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> {
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;input_ids&#34;</span>: input_ids,
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;attention_mask&#34;</span>: attention_mask,
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;labels&#34;</span>: labels
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>tokenized_ds <span style="color:#f92672">=</span> ds<span style="color:#f92672">.</span>map(process_func, remove_columns<span style="color:#f92672">=</span>ds<span style="color:#f92672">.</span>column_names)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 创建模型</span>
</span></span><span style="display:flex;"><span>model <span style="color:#f92672">=</span> AutoModelForCausalLM<span style="color:#f92672">.</span>from_pretrained(<span style="color:#e6db74">&#34;Langboat/bloom-1b4-zh&#34;</span>, low_cpu_mem_usage<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># BitFit 配置</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> name, param <span style="color:#f92672">in</span> model<span style="color:#f92672">.</span>named_parameters():
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> <span style="color:#e6db74">&#34;bias&#34;</span> <span style="color:#f92672">not</span> <span style="color:#f92672">in</span> name:
</span></span><span style="display:flex;"><span>        param<span style="color:#f92672">.</span>requires_grad <span style="color:#f92672">=</span> <span style="color:#66d9ef">False</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 配置训练参数</span>
</span></span><span style="display:flex;"><span>args <span style="color:#f92672">=</span> TrainingArguments(
</span></span><span style="display:flex;"><span>    output_dir<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;./chatbot&#34;</span>,
</span></span><span style="display:flex;"><span>    per_device_train_batch_size<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>,
</span></span><span style="display:flex;"><span>    gradient_accumulation_steps<span style="color:#f92672">=</span><span style="color:#ae81ff">8</span>,
</span></span><span style="display:flex;"><span>    logging_steps<span style="color:#f92672">=</span><span style="color:#ae81ff">10</span>,
</span></span><span style="display:flex;"><span>    num_train_epochs<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 创建训练器</span>
</span></span><span style="display:flex;"><span>trainer <span style="color:#f92672">=</span> Trainer(
</span></span><span style="display:flex;"><span>    model<span style="color:#f92672">=</span>model,
</span></span><span style="display:flex;"><span>    args<span style="color:#f92672">=</span>args,
</span></span><span style="display:flex;"><span>    tokenizer<span style="color:#f92672">=</span>tokenizer,
</span></span><span style="display:flex;"><span>    train_dataset<span style="color:#f92672">=</span>tokenized_ds,
</span></span><span style="display:flex;"><span>    data_collator<span style="color:#f92672">=</span>DataCollatorForSeq2Seq(tokenizer<span style="color:#f92672">=</span>tokenizer, padding<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>),
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 模型训练</span>
</span></span><span style="display:flex;"><span>trainer<span style="color:#f92672">.</span>train()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 模型推理</span>
</span></span><span style="display:flex;"><span>ipt <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;Human: </span><span style="color:#e6db74">{}</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">{}</span><span style="color:#e6db74">&#34;</span><span style="color:#f92672">.</span>format(<span style="color:#e6db74">&#34;考试有哪些技巧？&#34;</span>, <span style="color:#e6db74">&#34;&#34;</span>)<span style="color:#f92672">.</span>strip() <span style="color:#f92672">+</span> <span style="color:#e6db74">&#34;</span><span style="color:#ae81ff">\n\n</span><span style="color:#e6db74">Assistant: &#34;</span>
</span></span><span style="display:flex;"><span>output <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>generate(<span style="color:#f92672">**</span>tokenizer(ipt, return_tensors<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;pt&#34;</span>)<span style="color:#f92672">.</span>to(model<span style="color:#f92672">.</span>device), max_length<span style="color:#f92672">=</span><span style="color:#ae81ff">128</span>, do_sample<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>print(tokenizer<span style="color:#f92672">.</span>decode(output[<span style="color:#ae81ff">0</span>], skip_special_tokens<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>))
</span></span></code></pre></div><h3 id="解释-1">解释</h3>
<ul>
<li>在这个例子中，我们使用 BitFit 方法微调了一个 BLOOM 模型，只更新了模型中的偏置项。</li>
<li>这种方法在资源有限的情况下，能够有效地微调大型语言模型，同时保持较高的性能。</li>
</ul>
<h2 id="结论">结论</h2>
<p>BitFit 是一种非常实用的参数高效微调方法，特别适合在资源有限的情况下进行模型微调。通过只更新偏置项，BitFit 能够在保持模型性能的同时，显著减少计算和存储需求。在实际应用中，BitFit 可以帮助我们更高效地完成各种自然语言处理任务。</p>
</section>

  
  

  
  
  
  <nav class="mt-24 flex rounded-lg bg-black/[3%] text-lg dark:bg-white/[8%]">
    
    
    <a
      class="ml-auto flex w-1/2 items-center justify-end rounded-r-md p-6 pl-3 no-underline hover:bg-black/[2%]"
      href="https://869413421.github.io/post/transformer/chatbot_prefix_tuning/"
      ><span>Transformer 学习之路 - Prefix-Tuning 实战</span><span class="ml-1.5">→</span></a
    >
    
  </nav>
  

  
  

  
  
</article>


    </main>

    <footer
  class="opaco mx-auto flex h-[5rem] max-w-3xl items-center px-8 text-[0.9em] opacity-60"
>
  <div class="mr-auto">
    &copy; 2025
    <a class="link" href="https://869413421.github.io/">清水泥沙</a>
  </div>
</footer>

  </body>
</html>
