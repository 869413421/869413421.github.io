<!DOCTYPE html>






































<html
  class="not-ready text-sm lg:text-base"
  style="--bg: #fff"
  lang="en-zh"
>
  <head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta
    name="viewport"
    content="width=device-width, initial-scale=1, shrink-to-fit=no"
  />

  
  <title> - 清水泥沙</title>

  
  <meta name="theme-color" />
  
  <meta name="description" content="--- title: &#34;Transformer 学习之路 - 基于 Transformers 的命名实体识别&#34; date: 2024-04-19T17:35:18&#43;08:00 draft: false description: &#34;深入解析如何使用 Transformers 进行命名实体识别，涵盖技术原理、代码实现及应用场景&#34; categories: [&#34;Python&#34;, &#34;Transformer&#34;] --- # Transformer 学习之路 - 基于 Transformers 的命名实体识别 在自然语言处理（NLP）领域，命名实体识别（NER）是一项基础且重要的任务，旨在从文本中识别出特定类别的实体，如人名、地名、组织名等。本文将深入探讨如何使用 Transformers 库实现命名实体识别，并详细解析其技术原理和实现步骤。 ## 1. 背景与问题 命名实体识别是信息抽取的核心任务之一，广泛应用于知识图谱构建、问答系统、机器翻译等领域。传统方法依赖于规则和特征工程，而基于深度学习的 Transformer 模型通过自注意力机制，能够捕捉文本中的长距离依赖关系，显著提升了 NER 的性能。 ### 1.1 Transformer 的优势 - **自注意力机制**：Transformer 通过自注意力机制，能够同时关注输入序列中的每个位置，捕捉全局依赖关系。 - **并行计算**：与 RNN 不同，Transformer 无需按顺序处理输入，能够充分利用 GPU 的并行计算能力。 - **预训练模型**：通过大规模预训练，Transformer 模型能够学习到丰富的语言知识，适用于多种下游任务。 ## 2. 实现步骤 ### 2.1 环境准备 首先，安装必要的库： ```python ! pip install datasets evaluate seqeval 2.2 导入相关包 import evaluate from datasets import load_dataset from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer, DataCollatorForTokenClassification 2." />
  <meta
    name="author"
    content=""
  />
  

  
  
  
  
  
  
  <link rel="preload stylesheet" as="style" href="https://869413421.github.io/main.min.css" />

  

  
     
  <link rel="preload" as="image" href="https://869413421.github.io/theme.png" />

  
  
  
  <link rel="preload" as="image" href="https://www.gravatar.com/avatar/6fd8df4abe41f17fd8e2dd7d97b5cc8c?s=160&amp;d=identicon" />
  
  

  
  <link rel="preload" as="image" href="https://869413421.github.io/github.svg" />
  
  <link rel="preload" as="image" href="https://869413421.github.io/rss.svg" />
  

  
  <link rel="icon" href="https://869413421.github.io/favicon.ico" />
  <link rel="apple-touch-icon" href="https://869413421.github.io/apple-touch-icon.png" />

  
  <meta name="generator" content="Hugo 0.110.0">

  
  

  
  
  
  
  
  
  
  
  
  <meta property="og:title" content="" />
<meta property="og:description" content="--- title: &#34;Transformer 学习之路 - 基于 Transformers 的命名实体识别&#34; date: 2024-04-19T17:35:18&#43;08:00 draft: false description: &#34;深入解析如何使用 Transformers 进行命名实体识别，涵盖技术原理、代码实现及应用场景&#34; categories: [&#34;Python&#34;, &#34;Transformer&#34;] --- # Transformer 学习之路 - 基于 Transformers 的命名实体识别 在自然语言处理（NLP）领域，命名实体识别（NER）是一项基础且重要的任务，旨在从文本中识别出特定类别的实体，如人名、地名、组织名等。本文将深入探讨如何使用 Transformers 库实现命名实体识别，并详细解析其技术原理和实现步骤。 ## 1. 背景与问题 命名实体识别是信息抽取的核心任务之一，广泛应用于知识图谱构建、问答系统、机器翻译等领域。传统方法依赖于规则和特征工程，而基于深度学习的 Transformer 模型通过自注意力机制，能够捕捉文本中的长距离依赖关系，显著提升了 NER 的性能。 ### 1.1 Transformer 的优势 - **自注意力机制**：Transformer 通过自注意力机制，能够同时关注输入序列中的每个位置，捕捉全局依赖关系。 - **并行计算**：与 RNN 不同，Transformer 无需按顺序处理输入，能够充分利用 GPU 的并行计算能力。 - **预训练模型**：通过大规模预训练，Transformer 模型能够学习到丰富的语言知识，适用于多种下游任务。 ## 2. 实现步骤 ### 2.1 环境准备 首先，安装必要的库： ```python ! pip install datasets evaluate seqeval 2.2 导入相关包 import evaluate from datasets import load_dataset from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer, DataCollatorForTokenClassification 2." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://869413421.github.io/post/transformer/ner/" /><meta property="article:section" content="post" />



  
  <meta itemprop="name" content="">
<meta itemprop="description" content="--- title: &#34;Transformer 学习之路 - 基于 Transformers 的命名实体识别&#34; date: 2024-04-19T17:35:18&#43;08:00 draft: false description: &#34;深入解析如何使用 Transformers 进行命名实体识别，涵盖技术原理、代码实现及应用场景&#34; categories: [&#34;Python&#34;, &#34;Transformer&#34;] --- # Transformer 学习之路 - 基于 Transformers 的命名实体识别 在自然语言处理（NLP）领域，命名实体识别（NER）是一项基础且重要的任务，旨在从文本中识别出特定类别的实体，如人名、地名、组织名等。本文将深入探讨如何使用 Transformers 库实现命名实体识别，并详细解析其技术原理和实现步骤。 ## 1. 背景与问题 命名实体识别是信息抽取的核心任务之一，广泛应用于知识图谱构建、问答系统、机器翻译等领域。传统方法依赖于规则和特征工程，而基于深度学习的 Transformer 模型通过自注意力机制，能够捕捉文本中的长距离依赖关系，显著提升了 NER 的性能。 ### 1.1 Transformer 的优势 - **自注意力机制**：Transformer 通过自注意力机制，能够同时关注输入序列中的每个位置，捕捉全局依赖关系。 - **并行计算**：与 RNN 不同，Transformer 无需按顺序处理输入，能够充分利用 GPU 的并行计算能力。 - **预训练模型**：通过大规模预训练，Transformer 模型能够学习到丰富的语言知识，适用于多种下游任务。 ## 2. 实现步骤 ### 2.1 环境准备 首先，安装必要的库： ```python ! pip install datasets evaluate seqeval 2.2 导入相关包 import evaluate from datasets import load_dataset from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer, DataCollatorForTokenClassification 2.">

<meta itemprop="wordCount" content="321">
<meta itemprop="keywords" content="" />
  
  <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content=""/>
<meta name="twitter:description" content="--- title: &#34;Transformer 学习之路 - 基于 Transformers 的命名实体识别&#34; date: 2024-04-19T17:35:18&#43;08:00 draft: false description: &#34;深入解析如何使用 Transformers 进行命名实体识别，涵盖技术原理、代码实现及应用场景&#34; categories: [&#34;Python&#34;, &#34;Transformer&#34;] --- # Transformer 学习之路 - 基于 Transformers 的命名实体识别 在自然语言处理（NLP）领域，命名实体识别（NER）是一项基础且重要的任务，旨在从文本中识别出特定类别的实体，如人名、地名、组织名等。本文将深入探讨如何使用 Transformers 库实现命名实体识别，并详细解析其技术原理和实现步骤。 ## 1. 背景与问题 命名实体识别是信息抽取的核心任务之一，广泛应用于知识图谱构建、问答系统、机器翻译等领域。传统方法依赖于规则和特征工程，而基于深度学习的 Transformer 模型通过自注意力机制，能够捕捉文本中的长距离依赖关系，显著提升了 NER 的性能。 ### 1.1 Transformer 的优势 - **自注意力机制**：Transformer 通过自注意力机制，能够同时关注输入序列中的每个位置，捕捉全局依赖关系。 - **并行计算**：与 RNN 不同，Transformer 无需按顺序处理输入，能够充分利用 GPU 的并行计算能力。 - **预训练模型**：通过大规模预训练，Transformer 模型能够学习到丰富的语言知识，适用于多种下游任务。 ## 2. 实现步骤 ### 2.1 环境准备 首先，安装必要的库： ```python ! pip install datasets evaluate seqeval 2.2 导入相关包 import evaluate from datasets import load_dataset from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer, DataCollatorForTokenClassification 2."/>

  
  
</head>

  <body class="text-black duration-200 ease-out dark:text-white">
    <header class="mx-auto flex h-[5rem] max-w-3xl px-8 lg:justify-center">
  <div class="relative z-50 mr-auto flex items-center">
    <a
      class="-translate-x-[1px] -translate-y-0.5 text-3xl font-bold"
      href="https://869413421.github.io/"
      >清水泥沙</a
    >
    <a
      class="btn-dark ml-6 h-6 w-6 shrink-0 cursor-pointer [background:url(./theme.svg)_left_center/cover_no-repeat] dark:invert dark:[background-position:right]"
    ></a>
  </div>

  <a
    class="btn-menu relative z-50 -mr-8 flex h-[5rem] w-[5rem] shrink-0 cursor-pointer flex-col items-center justify-center gap-2.5 lg:hidden"
  ></a>

  
  <script>
    
    const htmlClass = document.documentElement.classList;
    setTimeout(() => {
      htmlClass.remove('not-ready');
    }, 10);

    
    const btnMenu = document.querySelector('.btn-menu');
    btnMenu.addEventListener('click', () => {
      htmlClass.toggle('open');
    });

    
    const metaTheme = document.querySelector('meta[name="theme-color"]');
    const lightBg = `"#fff"`.replace(/"/g, '');
    const setDark = (isDark) => {
      metaTheme.setAttribute('content', isDark ? '#000' : lightBg);
      htmlClass[isDark ? 'add' : 'remove']('dark');
      localStorage.setItem('dark', isDark);
    };

    
    const darkScheme = window.matchMedia('(prefers-color-scheme: dark)');
    if (htmlClass.contains('dark')) {
      setDark(true);
    } else {
      const darkVal = localStorage.getItem('dark');
      setDark(darkVal ? darkVal === 'true' : darkScheme.matches);
    }

    
    darkScheme.addEventListener('change', (event) => {
      setDark(event.matches);
    });

    
    const btnDark = document.querySelector('.btn-dark');
    btnDark.addEventListener('click', () => {
      setDark(localStorage.getItem('dark') !== 'true');
    });
  </script>

  <div
    class="nav-wrapper fixed inset-x-0 top-full z-40 flex h-full select-none flex-col justify-center pb-16 duration-200 dark:bg-black lg:static lg:h-auto lg:flex-row lg:!bg-transparent lg:pb-0 lg:transition-none"
  >
    
    
    <nav class="lg:ml-12 lg:flex lg:flex-row lg:items-center lg:space-x-6">
      
      <a
        class="block text-center text-2xl leading-[5rem] lg:text-base lg:font-normal"
        href="/tags"
        >标签</a
      >
      
      <a
        class="block text-center text-2xl leading-[5rem] lg:text-base lg:font-normal"
        href="/categories"
        >分类</a
      >
      
      <a
        class="block text-center text-2xl leading-[5rem] lg:text-base lg:font-normal"
        href="/about/"
        >个人简历</a
      >
      
    </nav>
    

    
    <nav
      class="mt-12 flex justify-center space-x-10 dark:invert lg:mt-0 lg:ml-12 lg:items-center lg:space-x-6"
    >
      
      <a
        class="h-8 w-8 [background:var(--url)_center_center/cover_no-repeat] lg:h-6 lg:w-6"
        style="--url: url(./github.svg)"
        href=" https://github.com/869413421 "
        target="_blank"
      ></a>
      
      <a
        class="h-8 w-8 [background:var(--url)_center_center/cover_no-repeat] lg:h-6 lg:w-6"
        style="--url: url(./rss.svg)"
        href=" https://869413421.github.io/index.xml "
        target="_blank"
      ></a>
      
    </nav>
    
  </div>
</header>


    <main
      class="prose prose-neutral relative mx-auto min-h-[calc(100%-10rem)] max-w-3xl px-8 pt-20 pb-32 dark:prose-invert"
    >
      

<article>
  <header class="mb-20">
    <h1 class="!my-0 pb-2.5"></h1>

    
    <div class="text-sm opacity-60">
      
      
    </div>
    
  </header>

  <section><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-markdown" data-lang="markdown"><span style="display:flex;"><span>---
</span></span><span style="display:flex;"><span>title: &#34;Transformer 学习之路 - 基于 Transformers 的命名实体识别&#34;
</span></span><span style="display:flex;"><span>date: 2024-04-19T17:35:18+08:00
</span></span><span style="display:flex;"><span>draft: false
</span></span><span style="display:flex;"><span>description: &#34;深入解析如何使用 Transformers 进行命名实体识别，涵盖技术原理、代码实现及应用场景&#34;
</span></span><span style="display:flex;"><span>categories: [&#34;Python&#34;, &#34;Transformer&#34;]
</span></span><span style="display:flex;"><span>---
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span># Transformer 学习之路 - 基于 Transformers 的命名实体识别
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>在自然语言处理（NLP）领域，命名实体识别（NER）是一项基础且重要的任务，旨在从文本中识别出特定类别的实体，如人名、地名、组织名等。本文将深入探讨如何使用 Transformers 库实现命名实体识别，并详细解析其技术原理和实现步骤。
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">## 1. 背景与问题
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>
</span></span><span style="display:flex;"><span>命名实体识别是信息抽取的核心任务之一，广泛应用于知识图谱构建、问答系统、机器翻译等领域。传统方法依赖于规则和特征工程，而基于深度学习的 Transformer 模型通过自注意力机制，能够捕捉文本中的长距离依赖关系，显著提升了 NER 的性能。
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">### 1.1 Transformer 的优势
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span><span style="color:#66d9ef">-</span> **自注意力机制**：Transformer 通过自注意力机制，能够同时关注输入序列中的每个位置，捕捉全局依赖关系。
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">-</span> **并行计算**：与 RNN 不同，Transformer 无需按顺序处理输入，能够充分利用 GPU 的并行计算能力。
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">-</span> **预训练模型**：通过大规模预训练，Transformer 模型能够学习到丰富的语言知识，适用于多种下游任务。
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">## 2. 实现步骤
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">### 2.1 环境准备
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>
</span></span><span style="display:flex;"><span>首先，安装必要的库：
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>```python
</span></span><span style="display:flex;"><span>! pip install datasets evaluate seqeval
</span></span></code></pre></div><h3 id="22-导入相关包">2.2 导入相关包</h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> evaluate
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> datasets <span style="color:#f92672">import</span> load_dataset
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> transformers <span style="color:#f92672">import</span> AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer, DataCollatorForTokenClassification
</span></span></code></pre></div><h3 id="23-加载数据集">2.3 加载数据集</h3>
<p>我们使用 <code>peoples_daily_ner</code> 数据集，该数据集包含中文文本及其对应的命名实体标签。</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>ner_datasets <span style="color:#f92672">=</span> load_dataset(<span style="color:#e6db74">&#34;peoples_daily_ner&#34;</span>)
</span></span></code></pre></div><p>查看数据集结构和标签映射：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>label_list <span style="color:#f92672">=</span> ner_datasets[<span style="color:#e6db74">&#39;train&#39;</span>]<span style="color:#f92672">.</span>features[<span style="color:#e6db74">&#39;ner_tags&#39;</span>]<span style="color:#f92672">.</span>feature<span style="color:#f92672">.</span>names
</span></span></code></pre></div><h3 id="24-数据预处理">2.4 数据预处理</h3>
<p>加载预训练的分词器，并对数据进行预处理：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>tokenizer <span style="color:#f92672">=</span> AutoTokenizer<span style="color:#f92672">.</span>from_pretrained(<span style="color:#e6db74">&#34;hfl/chinese-macbert-base&#34;</span>)
</span></span></code></pre></div><p>定义处理函数，将原始文本和标签映射为模型可接受的格式：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">process_function</span>(examples):
</span></span><span style="display:flex;"><span>    tokenized_exmaples <span style="color:#f92672">=</span> tokenizer(examples[<span style="color:#e6db74">&#34;tokens&#34;</span>], max_length<span style="color:#f92672">=</span><span style="color:#ae81ff">128</span>, truncation<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>, is_split_into_words<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>    labels <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> i, label <span style="color:#f92672">in</span> enumerate(examples[<span style="color:#e6db74">&#34;ner_tags&#34;</span>]):
</span></span><span style="display:flex;"><span>        word_ids <span style="color:#f92672">=</span> tokenized_exmaples<span style="color:#f92672">.</span>word_ids(batch_index<span style="color:#f92672">=</span>i)
</span></span><span style="display:flex;"><span>        label_ids <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> word_id <span style="color:#f92672">in</span> word_ids:
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">if</span> word_id <span style="color:#f92672">is</span> <span style="color:#66d9ef">None</span>:
</span></span><span style="display:flex;"><span>                label_ids<span style="color:#f92672">.</span>append(<span style="color:#f92672">-</span><span style="color:#ae81ff">100</span>)
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>                label_ids<span style="color:#f92672">.</span>append(label[word_id])
</span></span><span style="display:flex;"><span>        labels<span style="color:#f92672">.</span>append(label_ids)
</span></span><span style="display:flex;"><span>    tokenized_exmaples[<span style="color:#e6db74">&#34;labels&#34;</span>] <span style="color:#f92672">=</span> labels
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> tokenized_exmaples
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>tokenized_datasets <span style="color:#f92672">=</span> ner_datasets<span style="color:#f92672">.</span>map(process_function, batched<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span></code></pre></div><h3 id="25-创建模型">2.5 创建模型</h3>
<p>加载预训练模型，并指定标签数量：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>model <span style="color:#f92672">=</span> AutoModelForTokenClassification<span style="color:#f92672">.</span>from_pretrained(<span style="color:#e6db74">&#34;hfl/chinese-macbert-base&#34;</span>, num_labels<span style="color:#f92672">=</span>len(label_list))
</span></span></code></pre></div><h3 id="26-创建评估函数">2.6 创建评估函数</h3>
<p>使用 <code>seqeval</code> 库评估模型性能：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>seqeval <span style="color:#f92672">=</span> evaluate<span style="color:#f92672">.</span>load(<span style="color:#e6db74">&#34;seqeval&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">eval_metric</span>(pred):
</span></span><span style="display:flex;"><span>    predictions, labels <span style="color:#f92672">=</span> pred
</span></span><span style="display:flex;"><span>    predictions <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>argmax(predictions, axis<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>    true_predictions <span style="color:#f92672">=</span> [
</span></span><span style="display:flex;"><span>        [label_list[p] <span style="color:#66d9ef">for</span> p, l <span style="color:#f92672">in</span> zip(prediction, label) <span style="color:#66d9ef">if</span> l <span style="color:#f92672">!=</span> <span style="color:#f92672">-</span><span style="color:#ae81ff">100</span>]
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> prediction, label <span style="color:#f92672">in</span> zip(predictions, labels)
</span></span><span style="display:flex;"><span>    ]
</span></span><span style="display:flex;"><span>    true_labels <span style="color:#f92672">=</span> [
</span></span><span style="display:flex;"><span>        [label_list[l] <span style="color:#66d9ef">for</span> p, l <span style="color:#f92672">in</span> zip(prediction, label) <span style="color:#66d9ef">if</span> l <span style="color:#f92672">!=</span> <span style="color:#f92672">-</span><span style="color:#ae81ff">100</span>]
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> prediction, label <span style="color:#f92672">in</span> zip(predictions, labels)
</span></span><span style="display:flex;"><span>    ]
</span></span><span style="display:flex;"><span>    result <span style="color:#f92672">=</span> seqeval<span style="color:#f92672">.</span>compute(predictions<span style="color:#f92672">=</span>true_predictions, references<span style="color:#f92672">=</span>true_labels, mode<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;strict&#34;</span>, scheme<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;IOB2&#34;</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> {<span style="color:#e6db74">&#34;f1&#34;</span>: result[<span style="color:#e6db74">&#34;overall_f1&#34;</span>]}
</span></span></code></pre></div><h3 id="27-配置训练参数">2.7 配置训练参数</h3>
<p>设置训练参数，如批次大小、评估策略等：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>args <span style="color:#f92672">=</span> TrainingArguments(
</span></span><span style="display:flex;"><span>    output_dir<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;models_for_ner&#34;</span>,
</span></span><span style="display:flex;"><span>    per_device_train_batch_size<span style="color:#f92672">=</span><span style="color:#ae81ff">32</span>,
</span></span><span style="display:flex;"><span>    per_device_eval_batch_size<span style="color:#f92672">=</span><span style="color:#ae81ff">64</span>,
</span></span><span style="display:flex;"><span>    eval_strategy<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;epoch&#34;</span>,
</span></span><span style="display:flex;"><span>    save_strategy<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;epoch&#34;</span>,
</span></span><span style="display:flex;"><span>    metric_for_best_model<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;f1&#34;</span>,
</span></span><span style="display:flex;"><span>    load_best_model_at_end<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>,
</span></span><span style="display:flex;"><span>    logging_steps<span style="color:#f92672">=</span><span style="color:#ae81ff">50</span>,
</span></span><span style="display:flex;"><span>    num_train_epochs<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>)
</span></span></code></pre></div><h3 id="28-创建训练器">2.8 创建训练器</h3>
<p>初始化 <code>Trainer</code>，并传入模型、数据集和评估函数：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>trainer <span style="color:#f92672">=</span> Trainer(
</span></span><span style="display:flex;"><span>    model<span style="color:#f92672">=</span>model,
</span></span><span style="display:flex;"><span>    tokenizer<span style="color:#f92672">=</span>tokenizer,
</span></span><span style="display:flex;"><span>    args<span style="color:#f92672">=</span>args,
</span></span><span style="display:flex;"><span>    train_dataset<span style="color:#f92672">=</span>tokenized_datasets[<span style="color:#e6db74">&#34;train&#34;</span>],
</span></span><span style="display:flex;"><span>    eval_dataset<span style="color:#f92672">=</span>tokenized_datasets[<span style="color:#e6db74">&#34;validation&#34;</span>],
</span></span><span style="display:flex;"><span>    data_collator<span style="color:#f92672">=</span>DataCollatorForTokenClassification(tokenizer),
</span></span><span style="display:flex;"><span>    compute_metrics<span style="color:#f92672">=</span>eval_metric
</span></span><span style="display:flex;"><span>)
</span></span></code></pre></div><h3 id="29-执行训练">2.9 执行训练</h3>
<p>开始训练模型：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>trainer<span style="color:#f92672">.</span>train()
</span></span></code></pre></div><h3 id="210-模型评估">2.10 模型评估</h3>
<p>在测试集上评估模型性能：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>trainer<span style="color:#f92672">.</span>evaluate(eval_dataset<span style="color:#f92672">=</span>tokenized_datasets[<span style="color:#e6db74">&#34;test&#34;</span>])
</span></span></code></pre></div><h3 id="211-模型预测">2.11 模型预测</h3>
<p>使用训练好的模型进行预测：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> transformers <span style="color:#f92672">import</span> pipeline
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>id2label <span style="color:#f92672">=</span> {i: label <span style="color:#66d9ef">for</span> i, label <span style="color:#f92672">in</span> enumerate(label_list)}
</span></span><span style="display:flex;"><span>model<span style="color:#f92672">.</span>config<span style="color:#f92672">.</span>id2label <span style="color:#f92672">=</span> id2label
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>ner_pipe <span style="color:#f92672">=</span> pipeline(<span style="color:#e6db74">&#34;token-classification&#34;</span>, model<span style="color:#f92672">=</span>model, tokenizer<span style="color:#f92672">=</span>tokenizer, device<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>, aggregation_strategy<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;simple&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>sen <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;何智在广东省天河区附近的工商局上班打游戏&#34;</span>
</span></span><span style="display:flex;"><span>res <span style="color:#f92672">=</span> ner_pipe(sen)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>ner_result <span style="color:#f92672">=</span> {}
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> r <span style="color:#f92672">in</span> res:
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> r[<span style="color:#e6db74">&#34;entity_group&#34;</span>] <span style="color:#f92672">not</span> <span style="color:#f92672">in</span> ner_result:
</span></span><span style="display:flex;"><span>        ner_result[r[<span style="color:#e6db74">&#34;entity_group&#34;</span>]] <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>    ner_result[r[<span style="color:#e6db74">&#34;entity_group&#34;</span>]]<span style="color:#f92672">.</span>append(sen[r[<span style="color:#e6db74">&#34;start&#34;</span>]: r[<span style="color:#e6db74">&#34;end&#34;</span>]])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>ner_result
</span></span></code></pre></div><h2 id="3-总结">3. 总结</h2>
<p>本文详细介绍了如何使用 Transformers 库实现命名实体识别，涵盖了从数据加载、预处理、模型训练到评估和预测的全过程。通过 Transformer 模型，我们能够高效地处理复杂的文本数据，并从中提取出有用的信息。希望本文能为你提供有价值的参考，助你在 NLP 领域更进一步。</p>
<pre tabindex="0"><code></code></pre></section>

  
  

  
  
  
  <nav class="mt-24 flex rounded-lg bg-black/[3%] text-lg dark:bg-white/[8%]">
    
    <a
      class="flex w-1/2 items-center rounded-l-md p-6 pr-3 no-underline hover:bg-black/[2%]"
      href="https://869413421.github.io/post/transformer/multiple_choice/"
      ><span class="mr-1.5">←</span><span></span></a
    >
    
    
    <a
      class="ml-auto flex w-1/2 items-center justify-end rounded-r-md p-6 pl-3 no-underline hover:bg-black/[2%]"
      href="https://869413421.github.io/post/transformer/summarizatioin_glm/"
      ><span></span><span class="ml-1.5">→</span></a
    >
    
  </nav>
  

  
  

  
  
</article>


    </main>

    <footer
  class="opaco mx-auto flex h-[5rem] max-w-3xl items-center px-8 text-[0.9em] opacity-60"
>
  <div class="mr-auto">
    &copy; 2025
    <a class="link" href="https://869413421.github.io/">清水泥沙</a>
  </div>
</footer>

  </body>
</html>
